# Design a Chat System (1:1 + group) — full system design

Chat systems look simple (“send a message”), but they are a great system design test because they combine:

- realtime delivery (WebSockets)
- durable storage (message history)
- ordering and duplicates
- fanout (groups)
- mobile constraints (offline, push notifications)

This design aims for a practical, interview-ready solution with clear tradeoffs.

---

## 1) Requirements

### Functional requirements

- 1:1 chats and group chats
- Send and receive messages in near real time
- Message history (scroll back)
- Offline delivery (messages are delivered when user comes online)
- Multi-device support (phone + desktop)

### Nice-to-have (if time)

- Typing indicators
- Read receipts (“seen”)
- Presence (“online/offline/last seen”)
- Message edits/deletes
- Attachments (images/files)

### Non-functional requirements

- Low latency for online message delivery (p95 < ~200ms is typical target)
- High availability (chat should degrade gracefully)
- Durability (don’t lose messages)
- Scalability to large user base
- Security: auth, encryption in transit, abuse/spam controls

### Out of scope (unless asked)

- End-to-end encryption protocols (can mention as future work)
- Full media storage pipeline (can reuse “file storage” design)

---

## 2) Back-of-the-envelope scale (example)

Assume:

- 10M DAU
- average 30 messages/day/user → 300M messages/day
- average message size 300 bytes (text + metadata)

Then:

- messages/sec average ≈ 300M / 86,400 ≈ 3,472 msg/s
- peak factor 10× → ~35k msg/s peak
- raw ingest bandwidth ≈ 35k × 300B ≈ 10.5 MB/s (manageable)

Key takeaway:

- QPS is not insane; the hard parts are **fanout**, **ordering**, **storage model**, and **reliability**.

---

## 3) High-level architecture

### Components

- **API service** (HTTP): auth, inbox list, history fetch, metadata updates
- **Realtime gateway** (WebSocket): maintains connections, routes messages to online users
- **Message service**: validates messages, assigns IDs, persists, publishes events
- **Storage**:
  - metadata DB (users, conversations, memberships)
  - message store (optimized for “fetch messages by conversation”)
- **Event stream** (Kafka/PubSub): message events, delivery receipts, push notifications
- **Push notification service**: sends APNs/FCM notifications to offline devices
- **Presence service** (optional): tracks online/offline state (ephemeral)

### Key principle

Keep gateways **stateless** except for connection maps, so you can scale them horizontally.

---

## 4) API design (minimal and practical)

### Send message

`POST /v1/conversations/{conversation_id}/messages`

Request:

- `client_message_id` (UUID generated by client for dedupe)
- `text` (or attachment references)

Response:

- `server_message_id`
- `seq` (monotonic sequence per conversation)
- `created_at`

Why `client_message_id`:

- mobile networks retry; at-least-once delivery means duplicates can happen
- client IDs let server dedupe safely

### Realtime delivery

WebSocket events:

- `message.new`
- `message.ack` (server accepted)
- `message.delivered` (optional)
- `message.read` (optional)

### Fetch history (pagination)

`GET /v1/conversations/{conversation_id}/messages?before=<cursor>&limit=50`

Use cursor pagination using `(seq, server_message_id)` or `(created_at, id)`.

---

## 5) Data model

### Conversation metadata (SQL is fine)

- `users(user_id, ...)`
- `conversations(conversation_id, type, created_at, ...)`
- `memberships(conversation_id, user_id, role, joined_at, ...)`

### Message store (optimize for access pattern)

Hot query:

- “Fetch last 50 messages in a conversation”

A common schema in a wide-column store:

- Partition key: `conversation_id`
- Clustering: `seq` (increasing)

In SQL, you can do:

- `messages(conversation_id, seq, server_message_id, sender_id, created_at, body, ...)`
- primary key `(conversation_id, seq)` (or `(conversation_id, created_at, id)`)
- index for “latest” queries

Important: avoid global hot partitions; `conversation_id` partitions well, but large group chats can still become hot.

---

## 6) Ordering (the “seq per conversation” trick)

Users expect messages in a conversation to be in order.

Common approach:

- assign a **monotonic sequence number per conversation** on the server.

Where does `seq` come from?

Options:

- single leader per conversation partition (message service shard) maintains counter
- store `next_seq` in a DB row and update atomically (careful with contention)
- use a log/partition where ordering is naturally guaranteed (Kafka partition by conversation_id), then consumers assign seq

Interview-friendly approach:

- “We shard conversations across message service partitions. Each shard assigns increasing seq for conversations it owns.”

---

## 7) Delivery semantics (duplicates are normal)

Most chat systems use:

- **at-least-once** delivery internally (queues, retries)

So you must handle duplicates:

- server dedupes using `(conversation_id, client_message_id)`
- clients dedupe using `server_message_id`

This is a key senior correctness signal.

---

## 8) Online delivery (WebSocket routing)

### Connection routing

You need a mapping:

- `user_id -> list of active connections (gateway_id, connection_id)`

Store it in:

- in-memory on gateways + a shared directory (Redis) or
- a dedicated “connection directory” service

Flow:

1. User connects to WebSocket gateway (authenticated).
2. Gateway registers connection in directory with TTL heartbeat.
3. When a message arrives:
   - message service publishes event
   - a router/pusher component looks up recipients’ connections and pushes to correct gateways

### Scaling detail

Don’t broadcast to all gateways.

Use:

- partitioned directory
- targeted push (only gateways that host the user’s connections)

---

## 9) Offline delivery and push notifications

Offline delivery is achieved by:

- durable message storage (history)
- “last seen seq” per user per conversation

When user reconnects:

- client sends last seen `seq`
- server sends missing messages

Push notifications:

- when a message is stored, emit an event
- push service checks recipient preferences and sends APNs/FCM

Important:

- push is “best effort” and can be delayed; storage is the source of truth

---

## 10) Read receipts and presence (optional, but common)

### Read receipts

Model:

- `read_state(conversation_id, user_id, last_read_seq, updated_at)`

Tradeoff:

- read receipts can cause lots of writes in large groups

Optimization:

- enable only for small groups
- batch updates
- update “last read” no more than once every N seconds

### Presence

Presence is ephemeral.

Store in Redis with TTL:

- key: `presence:user:{id} = online`
- refresh via heartbeat

Avoid presence storms:

- don’t broadcast every heartbeat to everyone
- only update presence when it changes meaningfully

---

## 11) Large group chats (fanout and hot partitions)

Group chat can create “fanout to many recipients.”

Two patterns:

### A) Fanout on write (push to each member’s inbox)

- Pros: reads are fast
- Cons: write amplification for large groups

### B) Fanout on read (store once, members pull)

- Pros: writes are cheap
- Cons: reads require querying shared history + filtering

For chat, a common approach is:

- store message once per conversation
- deliver to online members via gateway push (realtime)
- offline members fetch from history

This avoids writing per-user inbox entries for massive groups.

---

## 12) Reliability and failure modes

### Gateway dies

- clients reconnect
- connection directory TTL expires old entry
- client resumes from last seen seq

### Message service dies

- writes fail temporarily; clients retry with same `client_message_id` (safe)

### Queue lag

- realtime delivery may be delayed
- but messages are stored, so correctness remains

### Duplicate delivery

- handled by dedupe logic

---

## 13) Security and abuse

- Authenticate WebSocket connections (short-lived tokens)
- Authorize conversation membership on send and read
- Rate limit sends per user to prevent spam
- Encrypt in transit (TLS)
- For E2E encryption, mention:
  - store encrypted payloads + key management (complex, out of scope)

---

## 14) Observability

Metrics:

- message send rate, delivery latency, p95/p99
- WebSocket connection count, reconnect rate
- queue lag, push notification success rate
- storage read/write latencies

Logs/tracing:

- correlation IDs per message (`server_message_id`)

---

## 15) Interview-ready summary (30 seconds)

“We use WebSocket gateways for realtime connections and a message service that validates, assigns message IDs and per-conversation sequence numbers, and persists messages in a store optimized for `conversation_id` reads. Delivery is at-least-once, so we dedupe using client-generated message IDs and server message IDs. Online users get pushed via gateway routing; offline users fetch from history and get push notifications via APNs/FCM. Presence/read receipts are optional and stored as lightweight state with batching to avoid write storms.”

